{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Import Libraries](#import)      \n",
    "- [Scraping Posts from Reddit](#scrape)\n",
    "- [Cleaning Data to Title + Text and Subreddit Only](#clean)\n",
    "- [Saving and Exporting csv for EDA](#save)\n",
    "\n",
    "Keeping scraping in an independent notebook such that when I restart kernels, I do NOT repull data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Importing Libraries <a id=\"import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import requests # Library enables requests from reddit\n",
    "import time # Library enables lag to be added to request loop\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Scraping Posts from Reddit <a id=\"scrape\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up source to scrape from\n",
    "\n",
    "url_base_ww = 'https://www.reddit.com/r/thewestwing.json' # API Endpoint 1\n",
    "url_base_nr = 'https://www.reddit.com/r/Thenewsroom.json' # API Endpoint 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function with a for loop to make pull requests from the sub reddit \n",
    "# until either 1000 posts have been pulled or the entire subreddit has been pulled\n",
    "\n",
    "def pull_posts(url_base):\n",
    "    \n",
    "    # Setting user agent to enable pulling from Reddit\n",
    "    user_agent = {'User-agent': 'mags'}\n",
    "    \n",
    "    # Instantiating 'after' string that can be concatenated to url after the first pull request\n",
    "    after = None\n",
    "    \n",
    "    # Creating empty posts list\n",
    "    posts = []\n",
    "    \n",
    "    for pull_req in range(int(1000/25)):\n",
    "        # Setting url to pull from based on whether it is the first pull (if)\n",
    "        # or a subsequent pull (else)\n",
    "        if after == None:\n",
    "            url = url_base\n",
    "        else:\n",
    "            url = url_base+\"?after=\"+after\n",
    "        \n",
    "        # Making request\n",
    "        res = requests.get(url, headers = user_agent)\n",
    "        \n",
    "        # If statement checks that request worked\n",
    "        # adds posts to posts list.\n",
    "        if res.status_code == 200:\n",
    "            json_data = res.json() \n",
    "            posts.extend(json_data['data']['children'])\n",
    "            # need to change after string such that next pull pulls the next set of posts\n",
    "            after = json_data['data']['after']\n",
    "            \n",
    "        else:\n",
    "            print(f'There was an error : {res.status_code}.')\n",
    "            break\n",
    "        \n",
    "        if after == None:\n",
    "            break\n",
    "        \n",
    "        print(f'Pulled {len(posts)} posts so far...')\n",
    "        time.sleep(5)\n",
    "    \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled 26 posts so far...\n",
      "Pulled 51 posts so far...\n",
      "Pulled 76 posts so far...\n",
      "Pulled 101 posts so far...\n",
      "Pulled 126 posts so far...\n",
      "Pulled 151 posts so far...\n",
      "Pulled 176 posts so far...\n",
      "Pulled 201 posts so far...\n",
      "Pulled 226 posts so far...\n",
      "Pulled 251 posts so far...\n",
      "Pulled 276 posts so far...\n",
      "Pulled 301 posts so far...\n",
      "Pulled 326 posts so far...\n",
      "Pulled 351 posts so far...\n",
      "Pulled 376 posts so far...\n",
      "Pulled 401 posts so far...\n",
      "Pulled 426 posts so far...\n",
      "Pulled 451 posts so far...\n",
      "Pulled 476 posts so far...\n",
      "Pulled 501 posts so far...\n",
      "Pulled 526 posts so far...\n",
      "Pulled 551 posts so far...\n",
      "Pulled 576 posts so far...\n",
      "Pulled 601 posts so far...\n",
      "Pulled 626 posts so far...\n",
      "Pulled 651 posts so far...\n",
      "Pulled 676 posts so far...\n",
      "Pulled 701 posts so far...\n",
      "Pulled 726 posts so far...\n",
      "Pulled 751 posts so far...\n",
      "Pulled 776 posts so far...\n",
      "Pulled 801 posts so far...\n",
      "Pulled 826 posts so far...\n",
      "Pulled 851 posts so far...\n",
      "Pulled 876 posts so far...\n",
      "Pulled 901 posts so far...\n",
      "Pulled 926 posts so far...\n",
      "Pulled 951 posts so far...\n",
      "Pulled 976 posts so far...\n"
     ]
    }
   ],
   "source": [
    "posts_ww = pull_posts(url_base_ww)\n",
    "ww_raw_df = pd.DataFrame(posts_ww)\n",
    "ww_raw_df.to_csv('./raw_ww.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled 25 posts so far...\n",
      "Pulled 50 posts so far...\n",
      "Pulled 75 posts so far...\n",
      "Pulled 100 posts so far...\n",
      "Pulled 125 posts so far...\n",
      "Pulled 150 posts so far...\n",
      "Pulled 175 posts so far...\n",
      "Pulled 200 posts so far...\n",
      "Pulled 225 posts so far...\n",
      "Pulled 250 posts so far...\n",
      "Pulled 275 posts so far...\n",
      "Pulled 300 posts so far...\n",
      "Pulled 325 posts so far...\n",
      "Pulled 350 posts so far...\n",
      "Pulled 375 posts so far...\n",
      "Pulled 400 posts so far...\n",
      "Pulled 425 posts so far...\n",
      "Pulled 450 posts so far...\n",
      "Pulled 475 posts so far...\n",
      "Pulled 500 posts so far...\n",
      "Pulled 525 posts so far...\n",
      "Pulled 550 posts so far...\n",
      "Pulled 575 posts so far...\n",
      "Pulled 600 posts so far...\n",
      "Pulled 625 posts so far...\n",
      "Pulled 650 posts so far...\n",
      "Pulled 675 posts so far...\n",
      "Pulled 700 posts so far...\n",
      "Pulled 725 posts so far...\n",
      "Pulled 750 posts so far...\n",
      "Pulled 775 posts so far...\n",
      "Pulled 800 posts so far...\n",
      "Pulled 825 posts so far...\n",
      "Pulled 850 posts so far...\n",
      "Pulled 875 posts so far...\n",
      "Pulled 900 posts so far...\n",
      "Pulled 925 posts so far...\n",
      "Pulled 950 posts so far...\n",
      "Pulled 975 posts so far...\n"
     ]
    }
   ],
   "source": [
    "posts_nr = pull_posts(url_base_nr)\n",
    "nr_raw_df = pd.DataFrame(posts_nr)\n",
    "nr_raw_df.to_csv('./raw_nr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cleaning Data to Title + Text and Subreddit Only <a id=\"clean\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to take the posts and format them such that \n",
    "# the title and text can be used for NLP.\n",
    "\n",
    "# Concatenating title and text because some posts do not have any text,\n",
    "# and some posts have short titles\n",
    "def clean_posts(posts):\n",
    "    # Creating empty list, one for titles and one for text\n",
    "    clean_text = []\n",
    "    \n",
    "    # Looping through each post to create dictionary\n",
    "    for data in posts:\n",
    "        # Creating empty dictionary\n",
    "        text = {}\n",
    "        \n",
    "        text['subreddit'] = data['data']['subreddit']\n",
    "        temp_text =  data['data']['title'] + \" \" + data['data']['selftext']\n",
    "        text = {text['subreddit'] : temp_text}\n",
    "        \n",
    "        clean_text.append(text)\n",
    "    \n",
    "    return pd.DataFrame(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ww = clean_posts(posts_ww)\n",
    "text_ww.to_csv('./titletext_ww.csv')\n",
    "text_ww.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_nr = clean_posts(posts_nr)\n",
    "text_nr.to_csv('./titletext_nr.csv')\n",
    "text_nr.columns = ['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Exporting CSV for EDA  <a id=\"save\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = 1 for West Wing, Y = 0 for Newsroom\n",
    "text_nr['subreddit'] = 0\n",
    "text_ww['subreddit'] = 1\n",
    "\n",
    "# Combining Newsroom and West Wing data frames \n",
    "text_df = text_nr.append(text_ww)\n",
    "text_df['class'] = text_df['subreddit']\n",
    "text_df.drop('subreddit', axis=1, inplace=True)\n",
    "\n",
    "text_df.to_csv('./text_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

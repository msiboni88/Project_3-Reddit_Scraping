{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents<a id=\"top\"></a>\n",
    "- [Importing Libraries](#import)\n",
    "- [Importing Scraped Data](#data)\n",
    "- [Count Vectorizer Models - No Stemming/Lemmetization, Stop Words Removed](#cvec1)\n",
    "- [TFIDF Vectorizer Models - No Stemming/Lemmetization, Stop Words Removed](#tvec1)\n",
    "- [Function to Tokenize, Lemmatize and Stem Posts](#func)\n",
    "- [Count Vectorizer Models - Lemmetized, Stop Words Removed](#cvec2)\n",
    "- [TFIDF Vectorizer Models - Lemmetized, Stop Words Removed](#tvec2)\n",
    "- [Count Vectorizer Models - Stemmed, Stop Words Removed](#cvec3) <-- Best Performing Vectorized Model\n",
    "- [TFIDF Vectorizer Models - Stemmed, Stop Words Removed](#tvec3)\n",
    "- [Naive Bayes Models](#nb)\n",
    "- [KNN, Random Forrest, SVM](#other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries <a id=\"import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Library tools to turn text in to interpretable DataFrames\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "# Object that uses count vectorizer and Logistic Regrssion as one\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Split data to check train model, Input parameters to create best model\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Importing lemmatizer.\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Importing stemmer.\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Import Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Naive Bayes Models \n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Scraped Data <a id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_csv('./text_df.csv')\n",
    "text_df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Unofficial Rewatch Thread - S3E02 \"Run\" **From TV Guide:** Rebecca defends ACN again as another lawsuit looms; Neal could be in trouble after a dangerous leak; Charlie and Leona confront a hostile takeover attempt by Reese\\'s half-siblings; Sloan worries that Don has crossed an ethical line; Hallie regrets a late-night tweet; Maggie weighs the pros and cons of eavesdropping.\\n\\n**From IMDb:** While Rebecca must once again defend ACN during a possible lawsuit, Will tries to protect Neal from the aftermath of the DOD leak; Charlie and Leona deal with a hostile takeover; Sloan worries about Don\\'s involvement with insider information.\\n\\n**Original Discussion Thread [HERE](https://www.reddit.com/r/Thenewsroom/comments/2mips3/episode_discussion_s03e02_run/)**'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['text'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer Models - No Stemming/Lemmetization, Stop Words Removed <a id=\"cvec1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "X = text_df['text']\n",
    "y = text_df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split data\n",
    "\n",
    "# Because the sample size is large and I noticed characters were being missed as indicators if they didn't make it \n",
    "# in to the training data, set test_size = 0.1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Pipeline for Count Vectorizer\n",
    "pipe = Pipeline([('cvec', CountVectorizer(stop_words='english')),\n",
    "                 ('lr', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8338907469342252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 2500,\n",
       " 'cvec__min_df': 3,\n",
       " 'cvec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Grid Search to create optimum  model\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [2500, 3000, 3500],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.846711259754738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.87,\n",
       " 'cvec__max_features': 2900,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReRun Grid Search based on previous results\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [2900, 3000, 3100],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.87, .9, .93],\n",
    "    'cvec__ngram_range': [(1,2)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8450390189520625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.87,\n",
       " 'cvec__max_features': 2900,\n",
       " 'cvec__min_df': 3,\n",
       " 'cvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReRun Grid Search based on previous results\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [2700, 2800, 2900],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.87],\n",
    "    'cvec__ngram_range': [(1,2)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=5)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.979933110367893.\n",
      "Test score: 0.865.\n"
     ]
    }
   ],
   "source": [
    "print(f'Train score: {gs.score(X_train, y_train)}.')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}.')\n",
    "\n",
    "# Model is overfit. How to fix? \n",
    "y_pred_test = gs.predict(X_test)\n",
    "y_pred_train = gs.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data frames to better understand the false predictions\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "train_df['text'] = X_train\n",
    "train_df['actual_y'] = y_train\n",
    "train_df['pred_y'] = y_pred_train\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "test_df['text'] = X_test\n",
    "test_df['actual_y'] = y_test\n",
    "test_df['pred_y'] = y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-201-725a6991b758>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_y\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactual_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_y\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactual_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 4375\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   4376\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 10"
     ]
    }
   ],
   "source": [
    "# Out of curiosity - Reading the incorrect predictions in the overfit train data. \n",
    "train_df[train_df.pred_y != train_df.actual_y]\n",
    "test_df[test_df.pred_y != test_df.actual_y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "just         588\n",
       "season       543\n",
       "episode      487\n",
       "like         416\n",
       "don          326\n",
       "think        326\n",
       "newsroom     299\n",
       "know         298\n",
       "sorkin       283\n",
       "president    278\n",
       "charlie      274\n",
       "time         268\n",
       "west         256\n",
       "wing         252\n",
       "west wing    251\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking to see what words are showing up. \n",
    "\n",
    "cvec_all_text = CountVectorizer(ngram_range=(1,2), max_features=3700, min_df=2, max_df=0.87, stop_words='english')\n",
    "\n",
    "cvec_all_text.fit(X_train)\n",
    "\n",
    "X_train_cv = cvec_all_text.transform(X_train)\n",
    "\n",
    "X_train_cv = pd.DataFrame(X_train_cv.toarray(),\n",
    "                                 columns = cvec_all_text.get_feature_names())\n",
    "\n",
    "X_train_cv.sum().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Vectorizer Models - No Stemming/Lemmetization, Stop Words Removed <a id=\"tvec1\"></a>\n",
    "\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Pipeline for TFID Vectorizer \n",
    "pipe = Pipeline([('tvec', TfidfVectorizer(stop_words='english')),\n",
    "                 ('lr', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8651059085841695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tvec__max_df': 0.7,\n",
       " 'tvec__max_features': 3000,\n",
       " 'tvec__min_df': 2,\n",
       " 'tvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'tvec__max_features': [2500, 3000, 3500],\n",
    "    'tvec__min_df': [2, 3],\n",
    "    'tvec__max_df': [.7, .8, .9],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8651059085841695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tvec__max_df': 0.3,\n",
       " 'tvec__max_features': 3000,\n",
       " 'tvec__min_df': 2,\n",
       " 'tvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'tvec__max_features': [2900, 3000, 3100],\n",
    "    'tvec__min_df': [2, 3],\n",
    "    'tvec__max_df': [.1, .3, .7],\n",
    "    'tvec__ngram_range': [(1,2)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8651059085841695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tvec__max_df': 0.3,\n",
       " 'tvec__max_features': 3000,\n",
       " 'tvec__min_df': 2,\n",
       " 'tvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'tvec__max_features': [3000],\n",
    "    'tvec__min_df': [2],\n",
    "    'tvec__max_df': [.2, .3, .4],\n",
    "    'tvec__ngram_range': [(1,2), (1,3)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9537346711259754.\n",
      "Test score: 0.875.\n"
     ]
    }
   ],
   "source": [
    "print(f'Train score: {gs.score(X_train, y_train)}.')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}.')\n",
    "\n",
    "# Model is overfit. How to fix? \n",
    "y_pred_test = gs.predict(X_test)\n",
    "y_pred_train = gs.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Tokenize, Lemmatize and Stem Posts<a id=\"func\"></a>\n",
    "\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562809726.7048068\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, lemmatizing, and stemming function \n",
    "import time\n",
    "\n",
    "# Instantiating\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "def lem_stem(df):\n",
    "    start = time.time()\n",
    "    print(start)\n",
    "    # Adding columns to DataFrame\n",
    "    df['lem_text'] = None\n",
    "    df['stem_text'] = None\n",
    "    \n",
    "    # Looping through each post\n",
    "    for i in range(df.shape[0]):\n",
    "        \n",
    "        post = df['text'][i]\n",
    "        tokens = tokenizer.tokenize(post.lower()) \n",
    "        \n",
    "        new_post_lem = ''\n",
    "        new_post_stem = ''\n",
    "        \n",
    "        for token in tokens:\n",
    "            lem = lemmatizer.lemmatize(token)\n",
    "            stem = p_stemmer.stem(token)\n",
    "            new_post_lem += ' ' + lem\n",
    "            new_post_stem += ' ' + stem\n",
    "            \n",
    "        df['lem_text'][i] = new_post_lem\n",
    "        df['stem_text'][i] = new_post_stem\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(f'{i} posts complete.')\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562809763.522989\n",
      "0 posts complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/mags/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 posts complete.\n",
      "200 posts complete.\n",
      "300 posts complete.\n",
      "400 posts complete.\n",
      "500 posts complete.\n",
      "600 posts complete.\n",
      "700 posts complete.\n",
      "800 posts complete.\n",
      "900 posts complete.\n",
      "1000 posts complete.\n",
      "1100 posts complete.\n",
      "1200 posts complete.\n",
      "1300 posts complete.\n",
      "1400 posts complete.\n",
      "1500 posts complete.\n",
      "1600 posts complete.\n",
      "1700 posts complete.\n",
      "1800 posts complete.\n",
      "1900 posts complete.\n",
      "1893.0269360542297\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>lem_text</th>\n",
       "      <th>stem_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Unofficial Rewatch Thread - S3E06: “What K...</td>\n",
       "      <td>0</td>\n",
       "      <td>the unofficial rewatch thread s3e06 what kind...</td>\n",
       "      <td>the unoffici rewatch thread s3e06 what kind o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just started watching it and have a few questi...</td>\n",
       "      <td>0</td>\n",
       "      <td>just started watching it and have a few quest...</td>\n",
       "      <td>just start watch it and have a few question i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Unofficial Rewatch Thread - S3E05: “Oh She...</td>\n",
       "      <td>0</td>\n",
       "      <td>the unofficial rewatch thread s3e05 oh shenan...</td>\n",
       "      <td>the unoffici rewatch thread s3e05 oh shenando...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On the bus, early season 2 Added the spoiler t...</td>\n",
       "      <td>0</td>\n",
       "      <td>on the bus early season 2 added the spoiler t...</td>\n",
       "      <td>on the bu earli season 2 ad the spoiler tag j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Unofficial Rewatch Thread: S3E04 - “Contem...</td>\n",
       "      <td>0</td>\n",
       "      <td>the unofficial rewatch thread s3e04 contempt ...</td>\n",
       "      <td>the unoffici rewatch thread s3e04 contempt fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What to watch after my 4th rewatch? It is one ...</td>\n",
       "      <td>0</td>\n",
       "      <td>what to watch after my 4th rewatch it is one ...</td>\n",
       "      <td>what to watch after my 4th rewatch it is one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Don Keefer's Greatest Hits</td>\n",
       "      <td>0</td>\n",
       "      <td>don keefer s greatest hit</td>\n",
       "      <td>don keefer s greatest hit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Unofficial Rewatch Thread - S3E03 \"Main Ju...</td>\n",
       "      <td>0</td>\n",
       "      <td>the unofficial rewatch thread s3e03 main just...</td>\n",
       "      <td>the unoffici rewatch thread s3e03 main justic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What welfare programs don’t work? Not sure on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>what welfare program don t work not sure on t...</td>\n",
       "      <td>what welfar program don t work not sure on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What did Will mean when he said he believed in...</td>\n",
       "      <td>0</td>\n",
       "      <td>what did will mean when he said he believed i...</td>\n",
       "      <td>what did will mean when he said he believ in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Unofficial Rewatch Thread - S3E02 \"Run\" **...</td>\n",
       "      <td>0</td>\n",
       "      <td>the unofficial rewatch thread s3e02 run from ...</td>\n",
       "      <td>the unoffici rewatch thread s3e02 run from tv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Unofficial Rewatch Thread - S3E01 \"Boston\"...</td>\n",
       "      <td>0</td>\n",
       "      <td>the unofficial rewatch thread s3e01 boston fr...</td>\n",
       "      <td>the unoffici rewatch thread s3e01 boston from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Watching for the first time Former local broad...</td>\n",
       "      <td>0</td>\n",
       "      <td>watching for the first time former local broa...</td>\n",
       "      <td>watch for the first time former local broadca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What software do they use for the news alerts,...</td>\n",
       "      <td>0</td>\n",
       "      <td>what software do they use for the news alert ...</td>\n",
       "      <td>what softwar do they use for the news alert o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>S3E06: What kind of day has it been. Don getti...</td>\n",
       "      <td>0</td>\n",
       "      <td>s3e06 what kind of day ha it been don getting...</td>\n",
       "      <td>s3e06 what kind of day ha it been don get the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Unofficial Rewatch Thread - S2E09: “Electi...</td>\n",
       "      <td>0</td>\n",
       "      <td>the unofficial rewatch thread s2e09 election ...</td>\n",
       "      <td>the unoffici rewatch thread s2e09 elect night...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Did this shoq make anyone wish they worked in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>did this shoq make anyone wish they worked in...</td>\n",
       "      <td>did thi shoq make anyon wish they work in a n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Three episodes left in my first watch and I st...</td>\n",
       "      <td>0</td>\n",
       "      <td>three episode left in my first watch and i st...</td>\n",
       "      <td>three episod left in my first watch and i sti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The Unofficial Rewatch Thread - S2E08: “Electi...</td>\n",
       "      <td>0</td>\n",
       "      <td>the unofficial rewatch thread s2e08 election ...</td>\n",
       "      <td>the unoffici rewatch thread s2e08 elect night...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The Unofficial Rewatch Thread - S2E07: “Red Te...</td>\n",
       "      <td>0</td>\n",
       "      <td>the unofficial rewatch thread s2e07 red team ...</td>\n",
       "      <td>the unoffici rewatch thread s2e07 red team ii...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>One thing that has always bugged me about 1-Ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>one thing that ha always bugged me about 1 ma...</td>\n",
       "      <td>one thing that ha alway bug me about 1 may au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Jeff Daniels in Rolling Stone: Stories From th...</td>\n",
       "      <td>0</td>\n",
       "      <td>jeff daniel in rolling stone story from the e...</td>\n",
       "      <td>jeff daniel in roll stone stori from the edg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The Unofficial Rewatch Thread - S2E06: “One St...</td>\n",
       "      <td>0</td>\n",
       "      <td>the unofficial rewatch thread s2e06 one step ...</td>\n",
       "      <td>the unoffici rewatch thread s2e06 one step to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TimesTalks: Aaron Sorkin and Jeff Daniels From...</td>\n",
       "      <td>0</td>\n",
       "      <td>timestalks aaron sorkin and jeff daniel from ...</td>\n",
       "      <td>timestalk aaron sorkin and jeff daniel from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The Unofficial Rewatch Thread - S2E05: “News N...</td>\n",
       "      <td>0</td>\n",
       "      <td>the unofficial rewatch thread s2e05 news nigh...</td>\n",
       "      <td>the unoffici rewatch thread s2e05 news night ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Newsroom OST? Does anyone know where I can fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>newsroom ost doe anyone know where i can find...</td>\n",
       "      <td>newsroom ost doe anyon know where i can find ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The Unofficial Rewatch Thread - S2E04: “Uninte...</td>\n",
       "      <td>0</td>\n",
       "      <td>the unofficial rewatch thread s2e04 unintende...</td>\n",
       "      <td>the unoffici rewatch thread s2e04 unintend co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Will's deposition came to mind.</td>\n",
       "      <td>0</td>\n",
       "      <td>will s deposition came to mind</td>\n",
       "      <td>will s deposit came to mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Se03 Ep02 - So, I still can’t find this laptop...</td>\n",
       "      <td>0</td>\n",
       "      <td>se03 ep02 so i still can t find this laptop h...</td>\n",
       "      <td>se03 ep02 so i still can t find thi laptop he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[SPOILER] Everything wrong with the Operation:...</td>\n",
       "      <td>0</td>\n",
       "      <td>spoiler everything wrong with the operation g...</td>\n",
       "      <td>spoiler everyth wrong with the oper genoa sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>In Excelsis Deo Watched this episode today, fe...</td>\n",
       "      <td>1</td>\n",
       "      <td>in excelsis deo watched this episode today fe...</td>\n",
       "      <td>in excelsi deo watch thi episod today felt fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>Question about Greg Brock and what would’ve ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>question about greg brock and what would ve h...</td>\n",
       "      <td>question about greg brock and what would ve h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>Larry and Ed I just watched the final episode ...</td>\n",
       "      <td>1</td>\n",
       "      <td>larry and ed i just watched the final episode...</td>\n",
       "      <td>larri and ed i just watch the final episod ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>Started a rewatch - was the remodel that was d...</td>\n",
       "      <td>1</td>\n",
       "      <td>started a rewatch wa the remodel that wa done...</td>\n",
       "      <td>start a rewatch wa the remodel that wa done l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>rain would not have stopped this president</td>\n",
       "      <td>1</td>\n",
       "      <td>rain would not have stopped this president</td>\n",
       "      <td>rain would not have stop thi presid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>Why was the lighting of the show so dark?</td>\n",
       "      <td>1</td>\n",
       "      <td>why wa the lighting of the show so dark</td>\n",
       "      <td>whi wa the light of the show so dark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>Is Charlie arrogant? I was wondering if anyone...</td>\n",
       "      <td>1</td>\n",
       "      <td>is charlie arrogant i wa wondering if anyone ...</td>\n",
       "      <td>is charli arrog i wa wonder if anyon els perc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>Sam ran just too early</td>\n",
       "      <td>1</td>\n",
       "      <td>sam ran just too early</td>\n",
       "      <td>sam ran just too earli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>Westwing easter eggs in Marvel's TV shows I ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>westwing easter egg in marvel s tv show i hav...</td>\n",
       "      <td>westw easter egg in marvel s tv show i have b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>Martin Sheen lost his home in the California w...</td>\n",
       "      <td>1</td>\n",
       "      <td>martin sheen lost his home in the california ...</td>\n",
       "      <td>martin sheen lost hi home in the california w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>\"I need you to jump off a cliff\"</td>\n",
       "      <td>1</td>\n",
       "      <td>i need you to jump off a cliff</td>\n",
       "      <td>i need you to jump off a cliff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>father complex All of the senior staff has som...</td>\n",
       "      <td>1</td>\n",
       "      <td>father complex all of the senior staff ha som...</td>\n",
       "      <td>father complex all of the senior staff ha som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>The administration's less notable moments Give...</td>\n",
       "      <td>1</td>\n",
       "      <td>the administration s le notable moment given ...</td>\n",
       "      <td>the administr s less notabl moment given the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>There's something I like about this playlist b...</td>\n",
       "      <td>1</td>\n",
       "      <td>there s something i like about this playlist ...</td>\n",
       "      <td>there s someth i like about thi playlist but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>Has anybody watched Mary McCormack (Kate Harpe...</td>\n",
       "      <td>1</td>\n",
       "      <td>ha anybody watched mary mccormack kate harper...</td>\n",
       "      <td>ha anybodi watch mari mccormack kate harper o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>Looking for a specific scene about a bipartisa...</td>\n",
       "      <td>1</td>\n",
       "      <td>looking for a specific scene about a bipartis...</td>\n",
       "      <td>look for a specif scene about a bipartisan me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>Will Bailey must have had a hand in this.</td>\n",
       "      <td>1</td>\n",
       "      <td>will bailey must have had a hand in this</td>\n",
       "      <td>will bailey must have had a hand in thi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>hoping to still be funded in 2019 budget</td>\n",
       "      <td>1</td>\n",
       "      <td>hoping to still be funded in 2019 budget</td>\n",
       "      <td>hope to still be fund in 2019 budget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>C.J. has a picture of Shirley Chisholm, first ...</td>\n",
       "      <td>1</td>\n",
       "      <td>c j ha a picture of shirley chisholm first bl...</td>\n",
       "      <td>c j ha a pictur of shirley chisholm first bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>A democrat from Texas running for president? S...</td>\n",
       "      <td>1</td>\n",
       "      <td>a democrat from texas running for president s...</td>\n",
       "      <td>a democrat from texa run for presid sure soun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>Today, in 2006, we lost Leo McGarry to a heart...</td>\n",
       "      <td>1</td>\n",
       "      <td>today in 2006 we lost leo mcgarry to a heart ...</td>\n",
       "      <td>today in 2006 we lost leo mcgarri to a heart ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>Did Will Bailey switch parties?</td>\n",
       "      <td>1</td>\n",
       "      <td>did will bailey switch party</td>\n",
       "      <td>did will bailey switch parti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>Congrats President Santos Florida Amendment 4 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>congrats president santos florida amendment 4...</td>\n",
       "      <td>congrat presid santo florida amend 4 pass con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>Sam?</td>\n",
       "      <td>1</td>\n",
       "      <td>sam</td>\n",
       "      <td>sam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>This feels like some awful, bizzaro world Will...</td>\n",
       "      <td>1</td>\n",
       "      <td>this feel like some awful bizzaro world will ...</td>\n",
       "      <td>thi feel like some aw bizzaro world will bail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>In 2016, Bill Clinton said he liked my shirt. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>in 2016 bill clinton said he liked my shirt t...</td>\n",
       "      <td>in 2016 bill clinton said he like my shirt to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>West Wing monologue for performance? We are wo...</td>\n",
       "      <td>1</td>\n",
       "      <td>west wing monologue for performance we are wo...</td>\n",
       "      <td>west wing monologu for perform we are work wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>dead candidate wins election - good job Will!</td>\n",
       "      <td>1</td>\n",
       "      <td>dead candidate win election good job will</td>\n",
       "      <td>dead candid win elect good job will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>Go vote... and don't tempt the wrath of the wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>go vote and don t tempt the wrath of the what...</td>\n",
       "      <td>go vote and don t tempt the wrath of the what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>Voted for your boy in all three boxes!</td>\n",
       "      <td>1</td>\n",
       "      <td>voted for your boy in all three box</td>\n",
       "      <td>vote for your boy in all three box</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1994 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  class  \\\n",
       "0     The Unofficial Rewatch Thread - S3E06: “What K...      0   \n",
       "1     Just started watching it and have a few questi...      0   \n",
       "2     The Unofficial Rewatch Thread - S3E05: “Oh She...      0   \n",
       "3     On the bus, early season 2 Added the spoiler t...      0   \n",
       "4     The Unofficial Rewatch Thread: S3E04 - “Contem...      0   \n",
       "5     What to watch after my 4th rewatch? It is one ...      0   \n",
       "6                           Don Keefer's Greatest Hits       0   \n",
       "7     The Unofficial Rewatch Thread - S3E03 \"Main Ju...      0   \n",
       "8     What welfare programs don’t work? Not sure on ...      0   \n",
       "9     What did Will mean when he said he believed in...      0   \n",
       "10    The Unofficial Rewatch Thread - S3E02 \"Run\" **...      0   \n",
       "11    The Unofficial Rewatch Thread - S3E01 \"Boston\"...      0   \n",
       "12    Watching for the first time Former local broad...      0   \n",
       "13    What software do they use for the news alerts,...      0   \n",
       "14    S3E06: What kind of day has it been. Don getti...      0   \n",
       "15    The Unofficial Rewatch Thread - S2E09: “Electi...      0   \n",
       "16    Did this shoq make anyone wish they worked in ...      0   \n",
       "17    Three episodes left in my first watch and I st...      0   \n",
       "18    The Unofficial Rewatch Thread - S2E08: “Electi...      0   \n",
       "19    The Unofficial Rewatch Thread - S2E07: “Red Te...      0   \n",
       "20    One thing that has always bugged me about 1-Ma...      0   \n",
       "21    Jeff Daniels in Rolling Stone: Stories From th...      0   \n",
       "22    The Unofficial Rewatch Thread - S2E06: “One St...      0   \n",
       "23    TimesTalks: Aaron Sorkin and Jeff Daniels From...      0   \n",
       "24    The Unofficial Rewatch Thread - S2E05: “News N...      0   \n",
       "25    Newsroom OST? Does anyone know where I can fin...      0   \n",
       "26    The Unofficial Rewatch Thread - S2E04: “Uninte...      0   \n",
       "27                     Will's deposition came to mind.       0   \n",
       "28    Se03 Ep02 - So, I still can’t find this laptop...      0   \n",
       "29    [SPOILER] Everything wrong with the Operation:...      0   \n",
       "...                                                 ...    ...   \n",
       "1964  In Excelsis Deo Watched this episode today, fe...      1   \n",
       "1965  Question about Greg Brock and what would’ve ha...      1   \n",
       "1966  Larry and Ed I just watched the final episode ...      1   \n",
       "1967  Started a rewatch - was the remodel that was d...      1   \n",
       "1968        rain would not have stopped this president       1   \n",
       "1969         Why was the lighting of the show so dark?       1   \n",
       "1970  Is Charlie arrogant? I was wondering if anyone...      1   \n",
       "1971                            Sam ran just too early       1   \n",
       "1972  Westwing easter eggs in Marvel's TV shows I ha...      1   \n",
       "1973  Martin Sheen lost his home in the California w...      1   \n",
       "1974                  \"I need you to jump off a cliff\"       1   \n",
       "1975  father complex All of the senior staff has som...      1   \n",
       "1976  The administration's less notable moments Give...      1   \n",
       "1977  There's something I like about this playlist b...      1   \n",
       "1978  Has anybody watched Mary McCormack (Kate Harpe...      1   \n",
       "1979  Looking for a specific scene about a bipartisa...      1   \n",
       "1980         Will Bailey must have had a hand in this.       1   \n",
       "1981          hoping to still be funded in 2019 budget       1   \n",
       "1982  C.J. has a picture of Shirley Chisholm, first ...      1   \n",
       "1983  A democrat from Texas running for president? S...      1   \n",
       "1984  Today, in 2006, we lost Leo McGarry to a heart...      1   \n",
       "1985                   Did Will Bailey switch parties?       1   \n",
       "1986  Congrats President Santos Florida Amendment 4 ...      1   \n",
       "1987                                              Sam?       1   \n",
       "1988  This feels like some awful, bizzaro world Will...      1   \n",
       "1989  In 2016, Bill Clinton said he liked my shirt. ...      1   \n",
       "1990  West Wing monologue for performance? We are wo...      1   \n",
       "1991     dead candidate wins election - good job Will!       1   \n",
       "1992  Go vote... and don't tempt the wrath of the wh...      1   \n",
       "1993            Voted for your boy in all three boxes!       1   \n",
       "\n",
       "                                               lem_text  \\\n",
       "0      the unofficial rewatch thread s3e06 what kind...   \n",
       "1      just started watching it and have a few quest...   \n",
       "2      the unofficial rewatch thread s3e05 oh shenan...   \n",
       "3      on the bus early season 2 added the spoiler t...   \n",
       "4      the unofficial rewatch thread s3e04 contempt ...   \n",
       "5      what to watch after my 4th rewatch it is one ...   \n",
       "6                             don keefer s greatest hit   \n",
       "7      the unofficial rewatch thread s3e03 main just...   \n",
       "8      what welfare program don t work not sure on t...   \n",
       "9      what did will mean when he said he believed i...   \n",
       "10     the unofficial rewatch thread s3e02 run from ...   \n",
       "11     the unofficial rewatch thread s3e01 boston fr...   \n",
       "12     watching for the first time former local broa...   \n",
       "13     what software do they use for the news alert ...   \n",
       "14     s3e06 what kind of day ha it been don getting...   \n",
       "15     the unofficial rewatch thread s2e09 election ...   \n",
       "16     did this shoq make anyone wish they worked in...   \n",
       "17     three episode left in my first watch and i st...   \n",
       "18     the unofficial rewatch thread s2e08 election ...   \n",
       "19     the unofficial rewatch thread s2e07 red team ...   \n",
       "20     one thing that ha always bugged me about 1 ma...   \n",
       "21     jeff daniel in rolling stone story from the e...   \n",
       "22     the unofficial rewatch thread s2e06 one step ...   \n",
       "23     timestalks aaron sorkin and jeff daniel from ...   \n",
       "24     the unofficial rewatch thread s2e05 news nigh...   \n",
       "25     newsroom ost doe anyone know where i can find...   \n",
       "26     the unofficial rewatch thread s2e04 unintende...   \n",
       "27                       will s deposition came to mind   \n",
       "28     se03 ep02 so i still can t find this laptop h...   \n",
       "29     spoiler everything wrong with the operation g...   \n",
       "...                                                 ...   \n",
       "1964   in excelsis deo watched this episode today fe...   \n",
       "1965   question about greg brock and what would ve h...   \n",
       "1966   larry and ed i just watched the final episode...   \n",
       "1967   started a rewatch wa the remodel that wa done...   \n",
       "1968         rain would not have stopped this president   \n",
       "1969            why wa the lighting of the show so dark   \n",
       "1970   is charlie arrogant i wa wondering if anyone ...   \n",
       "1971                             sam ran just too early   \n",
       "1972   westwing easter egg in marvel s tv show i hav...   \n",
       "1973   martin sheen lost his home in the california ...   \n",
       "1974                     i need you to jump off a cliff   \n",
       "1975   father complex all of the senior staff ha som...   \n",
       "1976   the administration s le notable moment given ...   \n",
       "1977   there s something i like about this playlist ...   \n",
       "1978   ha anybody watched mary mccormack kate harper...   \n",
       "1979   looking for a specific scene about a bipartis...   \n",
       "1980           will bailey must have had a hand in this   \n",
       "1981           hoping to still be funded in 2019 budget   \n",
       "1982   c j ha a picture of shirley chisholm first bl...   \n",
       "1983   a democrat from texas running for president s...   \n",
       "1984   today in 2006 we lost leo mcgarry to a heart ...   \n",
       "1985                       did will bailey switch party   \n",
       "1986   congrats president santos florida amendment 4...   \n",
       "1987                                                sam   \n",
       "1988   this feel like some awful bizzaro world will ...   \n",
       "1989   in 2016 bill clinton said he liked my shirt t...   \n",
       "1990   west wing monologue for performance we are wo...   \n",
       "1991          dead candidate win election good job will   \n",
       "1992   go vote and don t tempt the wrath of the what...   \n",
       "1993                voted for your boy in all three box   \n",
       "\n",
       "                                              stem_text  \n",
       "0      the unoffici rewatch thread s3e06 what kind o...  \n",
       "1      just start watch it and have a few question i...  \n",
       "2      the unoffici rewatch thread s3e05 oh shenando...  \n",
       "3      on the bu earli season 2 ad the spoiler tag j...  \n",
       "4      the unoffici rewatch thread s3e04 contempt fr...  \n",
       "5      what to watch after my 4th rewatch it is one ...  \n",
       "6                             don keefer s greatest hit  \n",
       "7      the unoffici rewatch thread s3e03 main justic...  \n",
       "8      what welfar program don t work not sure on th...  \n",
       "9      what did will mean when he said he believ in ...  \n",
       "10     the unoffici rewatch thread s3e02 run from tv...  \n",
       "11     the unoffici rewatch thread s3e01 boston from...  \n",
       "12     watch for the first time former local broadca...  \n",
       "13     what softwar do they use for the news alert o...  \n",
       "14     s3e06 what kind of day ha it been don get the...  \n",
       "15     the unoffici rewatch thread s2e09 elect night...  \n",
       "16     did thi shoq make anyon wish they work in a n...  \n",
       "17     three episod left in my first watch and i sti...  \n",
       "18     the unoffici rewatch thread s2e08 elect night...  \n",
       "19     the unoffici rewatch thread s2e07 red team ii...  \n",
       "20     one thing that ha alway bug me about 1 may au...  \n",
       "21     jeff daniel in roll stone stori from the edg ...  \n",
       "22     the unoffici rewatch thread s2e06 one step to...  \n",
       "23     timestalk aaron sorkin and jeff daniel from t...  \n",
       "24     the unoffici rewatch thread s2e05 news night ...  \n",
       "25     newsroom ost doe anyon know where i can find ...  \n",
       "26     the unoffici rewatch thread s2e04 unintend co...  \n",
       "27                          will s deposit came to mind  \n",
       "28     se03 ep02 so i still can t find thi laptop he...  \n",
       "29     spoiler everyth wrong with the oper genoa sto...  \n",
       "...                                                 ...  \n",
       "1964   in excelsi deo watch thi episod today felt fi...  \n",
       "1965   question about greg brock and what would ve h...  \n",
       "1966   larri and ed i just watch the final episod ag...  \n",
       "1967   start a rewatch wa the remodel that wa done l...  \n",
       "1968                rain would not have stop thi presid  \n",
       "1969               whi wa the light of the show so dark  \n",
       "1970   is charli arrog i wa wonder if anyon els perc...  \n",
       "1971                             sam ran just too earli  \n",
       "1972   westw easter egg in marvel s tv show i have b...  \n",
       "1973   martin sheen lost hi home in the california w...  \n",
       "1974                     i need you to jump off a cliff  \n",
       "1975   father complex all of the senior staff ha som...  \n",
       "1976   the administr s less notabl moment given the ...  \n",
       "1977   there s someth i like about thi playlist but ...  \n",
       "1978   ha anybodi watch mari mccormack kate harper o...  \n",
       "1979   look for a specif scene about a bipartisan me...  \n",
       "1980            will bailey must have had a hand in thi  \n",
       "1981               hope to still be fund in 2019 budget  \n",
       "1982   c j ha a pictur of shirley chisholm first bla...  \n",
       "1983   a democrat from texa run for presid sure soun...  \n",
       "1984   today in 2006 we lost leo mcgarri to a heart ...  \n",
       "1985                       did will bailey switch parti  \n",
       "1986   congrat presid santo florida amend 4 pass con...  \n",
       "1987                                                sam  \n",
       "1988   thi feel like some aw bizzaro world will bail...  \n",
       "1989   in 2016 bill clinton said he like my shirt to...  \n",
       "1990   west wing monologu for perform we are work wi...  \n",
       "1991                dead candid win elect good job will  \n",
       "1992   go vote and don t tempt the wrath of the what...  \n",
       "1993                 vote for your boy in all three box  \n",
       "\n",
       "[1994 rows x 4 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Commenting out calling this code because it was extremely time intensive and I do not want to run it again \n",
    "# by mistake. If I could spend more time on this project, I would like to find a way to do this that is less\n",
    "# computationally intensive.\n",
    "\n",
    "# lem_stem(text_df)\n",
    "# text_df.to_csv('./lem_stem.csv')\n",
    "\n",
    "# Saving csv and loading below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-f6fd1656d79b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'start' is not defined"
     ]
    }
   ],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_stem_df = pd.read_csv('./lem_stem.csv')\n",
    "lem_stem_df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer Models - Lemmetized, Stop Words Removed <a id=\"cvec2\"></a>\n",
    "\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lem_stem_df['lem_text']\n",
    "y = lem_stem_df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cvec', CountVectorizer(stop_words='english')),\n",
    "                 ('lr', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8311036789297659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 3000,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [2500, 3000, 3500],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8311036789297659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.75,\n",
       " 'cvec__max_features': 3000,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [2900, 3000, 3100],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.75, .85, .9],\n",
    "    'cvec__ngram_range': [(1,2)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8355629877369007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.15,\n",
       " 'cvec__max_features': 3000,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [3000],\n",
    "    'cvec__min_df': [2],\n",
    "    'cvec__max_df': [.15, .35, .75],\n",
    "    'cvec__ngram_range': [(1,2)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8355629877369007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.15,\n",
       " 'cvec__max_features': 3000,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [3000],\n",
    "    'cvec__min_df': [2],\n",
    "    'cvec__max_df': [.14, .15, .16],\n",
    "    'cvec__ngram_range': [(1,2)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9788182831661093.\n",
      "Test score: 0.855.\n"
     ]
    }
   ],
   "source": [
    "print(f'Train score: {gs.score(X_train, y_train)}.')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}.')\n",
    "\n",
    "# Model accuracy is almost identical to non lemmetized model.\n",
    "y_pred_test = gs.predict(X_test)\n",
    "y_pred_train = gs.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Vectorizer Models - Lemmetized, Stop Words Removed <a id=\"tvec2\"></a>\n",
    "\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('tvec', TfidfVectorizer(stop_words='english')),\n",
    "                 ('lr', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8584169453734671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tvec__max_df': 0.3,\n",
       " 'tvec__max_features': 3000,\n",
       " 'tvec__min_df': 2,\n",
       " 'tvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'tvec__max_features': [2000, 3000],\n",
    "    'tvec__min_df': [2],\n",
    "    'tvec__max_df': [.2, .3, .4],\n",
    "    'tvec__ngram_range': [(1,2), (1,3)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8606465997770345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tvec__max_df': 0.25,\n",
       " 'tvec__max_features': 3100,\n",
       " 'tvec__min_df': 2,\n",
       " 'tvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'tvec__max_features': [2900, 3000, 3100],\n",
    "    'tvec__min_df': [2, 3],\n",
    "    'tvec__max_df': [.25, .3],\n",
    "    'tvec__ngram_range': [(1,2)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8606465997770345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tvec__max_df': 0.25,\n",
       " 'tvec__max_features': 3100,\n",
       " 'tvec__min_df': 2,\n",
       " 'tvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'tvec__max_features': [3100, 3200, 3300],\n",
    "    'tvec__min_df': [2],\n",
    "    'tvec__max_df': [.1, .2, .25],\n",
    "    'tvec__ngram_range': [(1,2)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9559643255295429.\n",
      "Test score: 0.875.\n"
     ]
    }
   ],
   "source": [
    "print(f'Train score: {gs.score(X_train, y_train)}.')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}.')\n",
    "\n",
    "# Model accuracy is almost identical to non lemmetized model, just slightly better\n",
    "y_pred_test = gs.predict(X_test)\n",
    "y_pred_train = gs.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer Models - Stemmed, Stop Words Removed <a id=\"cvec3\"></a>\n",
    "\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lem_stem_df['stem_text']\n",
    "y = lem_stem_df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cvec', CountVectorizer(stop_words='english')),\n",
    "                 ('lr', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8327759197324415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.3,\n",
       " 'cvec__max_features': 4000,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [2000, 3000, 4000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.05, .15, .3],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.23,\n",
       " 'cvec__max_features': 4000,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [3500, 4000, 4500],\n",
    "    'cvec__min_df': [2],\n",
    "    'cvec__max_df': [.23, .3, .35],\n",
    "    'cvec__ngram_range': [(1,2)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8472686733556298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.23,\n",
       " 'cvec__max_features': 4050,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'lr__C': 0.08,\n",
       " 'lr__penalty': 'l2'}"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [4050],\n",
    "    'cvec__min_df': [2],\n",
    "    'cvec__max_df': [.22, .23, .24],\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'lr__C' : [.08],\n",
    "    'lr__penalty': ['l2']\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.947603121516165.\n",
      "Test score: 0.88.\n"
     ]
    }
   ],
   "source": [
    "print(f'Train score: {gs.score(X_train, y_train)}.')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}.')\n",
    "\n",
    "# Best version of three very similar models\n",
    "y_pred_test = gs.predict(X_test)\n",
    "y_pred_train = gs.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds['lr'] = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Vectorizer Models - Stemmed, Stop Words Removed <a id=\"tvec3\"></a>\n",
    "\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('tvec', TfidfVectorizer(stop_words='english')),\n",
    "                 ('lr', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8578595317725752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tvec__max_df': 0.3,\n",
       " 'tvec__max_features': 3100,\n",
       " 'tvec__min_df': 3,\n",
       " 'tvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'tvec__max_features': [2600, 3100, 3600],\n",
    "    'tvec__min_df': [2, 3],\n",
    "    'tvec__max_df': [.2, .25, .3],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mags/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8578595317725752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tvec__max_df': 0.29,\n",
       " 'tvec__max_features': 2950,\n",
       " 'tvec__min_df': 3,\n",
       " 'tvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'tvec__max_features': [2700, 2800, 2950],\n",
    "    'tvec__min_df': [3],\n",
    "    'tvec__max_df': [.27, .28, .29],\n",
    "    'tvec__ngram_range': [(1,2)]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.955406911928651.\n",
      "Test score: 0.875.\n"
     ]
    }
   ],
   "source": [
    "print(f'Train score: {gs.score(X_train, y_train)}.')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}.')\n",
    "\n",
    "# Identical performance to lemmetized data\n",
    "y_pred_test = gs.predict(X_test)\n",
    "y_pred_train = gs.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model <a id=\"nb\"></a>\n",
    "\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models to run\n",
    "I will be creating 6 total Naive Bayes models to compare based on my best found hyperparameters for each of the 6 types of Vectorized models that I ran. I will be running MultinomialNB on my count vectorizer models and GaussianNB on my TFIDF models.\n",
    "\n",
    "- Count Vectorizer - No Stemming/Lemmetization, Stop Words Removed\n",
    "    - {'cvec__max_df': 0.87, 'cvec__max_features': 2900, 'cvec__min_df': 3, 'cvec__ngram_range': (1, 2)}\n",
    "    - Train accuracy score = 98.0%\n",
    "    - Test accuracy score = 86.5%\n",
    "- TFIDF Vectorizer - No Stemming/Lemmetization, Stop Words Removed\n",
    "    - {'tvec__max_df': 0.3, 'tvec__max_features': 3000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 2)}\n",
    "    - Train accuracy score = 95.3%\n",
    "    - Test accuracy score = 87.5%\n",
    "- Count Vectorizer - Lemmetized, Stop Words Removed\n",
    "    - {'cvec__max_df': 0.15, 'cvec__max_features': 3000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2)}\n",
    "    - Train accuracy score = 97.9%\n",
    "    - Test accuracy score = 85.5%\n",
    "- TFIDF Vectorizer - Lemmetized, Stop Words Removed\n",
    "    - {'tvec__max_df': 0.25, 'tvec__max_features': 3100, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 2)}\n",
    "    - Train accuracy score = 95.6%\n",
    "    - Test accuracy score = 87.5%\n",
    "- Count Vectorizer - Stemmed, Stop Words Removed <-- Best Performing Vectorized Model\n",
    "    - {'cvec__max_df': 0.23, 'cvec__max_features': 4100, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2)}\n",
    "    - Train accuracy score = 98.5%\n",
    "    - Test accuracy score = 87.5%\n",
    "- TFIDF Vectorizer - Stemmed, Stop Words Removed\n",
    "    - {'tvec__max_df': 0.29, 'tvec__max_features': 2950, 'tvec__min_df': 3, 'tvec__ngram_range': (1, 2)}\n",
    "    - Train accuracy score = 95.5%\n",
    "    - Test accuracy score = 87.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9241917502787068\n",
      "Test score: 0.885\n"
     ]
    }
   ],
   "source": [
    "X = lem_stem_df['text']\n",
    "y = lem_stem_df['class']\n",
    "\n",
    "vect = CountVectorizer(max_df=0.87, max_features=2900, ngram_range=(1,2), stop_words='english')\n",
    "vect.fit(X)\n",
    "\n",
    "X_transform = vect.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transform, y, stratify=y, test_size=0.1, random_state=30)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "print(f'Train score: {metrics.accuracy_score(y_train, nb.predict(X_train))}')\n",
    "print(f'Test score: {metrics.accuracy_score(y_test, nb.predict(X_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9437012263099219\n",
      "Test score: 0.775\n"
     ]
    }
   ],
   "source": [
    "tvect = TfidfVectorizer(max_df=0.3, max_features=3000, min_df=2, ngram_range=(1,2), stop_words='english')\n",
    "tvect.fit(X)\n",
    "\n",
    "X_transform = tvect.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transform, y, stratify=y, test_size=0.1, random_state=30)\n",
    "\n",
    "X_train_dense = X_train.toarray()\n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "gb = GaussianNB()\n",
    "gb.fit(X_train_dense, y_train)\n",
    "\n",
    "print(f'Train score: {metrics.accuracy_score(y_train, gb.predict(X_train_dense))}')\n",
    "print(f'Test score: {metrics.accuracy_score(y_test, gb.predict(X_test_dense))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9319955406911928\n",
      "Test score: 0.885\n"
     ]
    }
   ],
   "source": [
    "X = lem_stem_df['lem_text']\n",
    "y = lem_stem_df['class']\n",
    "\n",
    "vect = CountVectorizer(max_df=0.15, max_features=3000, min_df=2, ngram_range=(1,2), stop_words='english')\n",
    "vect.fit(X)\n",
    "\n",
    "X_transform = vect.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transform, y, stratify=y, test_size=0.1, random_state=30)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "print(f'Train score: {metrics.accuracy_score(y_train, nb.predict(X_train))}')\n",
    "print(f'Test score: {metrics.accuracy_score(y_test, nb.predict(X_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9453734671125975\n",
      "Test score: 0.755\n"
     ]
    }
   ],
   "source": [
    "tvect = TfidfVectorizer(max_df=0.25, max_features=3100, min_df=2, ngram_range=(1,2), stop_words='english')\n",
    "tvect.fit(X)\n",
    "\n",
    "X_transform = tvect.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transform, y, stratify=y, test_size=0.1, random_state=30)\n",
    "\n",
    "X_train_dense = X_train.toarray()\n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "gb = GaussianNB()\n",
    "gb.fit(X_train_dense, y_train)\n",
    "\n",
    "print(f'Train score: {metrics.accuracy_score(y_train, gb.predict(X_train_dense))}')\n",
    "print(f'Test score: {metrics.accuracy_score(y_test, gb.predict(X_test_dense))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9375696767001115\n",
      "Test score: 0.885\n"
     ]
    }
   ],
   "source": [
    "X = lem_stem_df['stem_text']\n",
    "y = lem_stem_df['class']\n",
    "\n",
    "vect = CountVectorizer(max_df=0.23, max_features=4100, min_df=2, ngram_range=(1,2), stop_words='english')\n",
    "vect.fit(X)\n",
    "\n",
    "X_transform = vect.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transform, y, stratify=y, test_size=0.1, random_state=30)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "print(f'Train score: {metrics.accuracy_score(y_train, nb.predict(X_train))}')\n",
    "print(f'Test score: {metrics.accuracy_score(y_test, nb.predict(X_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds['nb'] = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9509476031215162\n",
      "Test score: 0.74\n"
     ]
    }
   ],
   "source": [
    "tvect = TfidfVectorizer(max_df=0.29, max_features=2950, min_df=3, ngram_range=(1,2), stop_words='english')\n",
    "tvect.fit(X)\n",
    "\n",
    "X_transform = tvect.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transform, y, stratify=y, test_size=0.1, random_state=30)\n",
    "\n",
    "X_train_dense = X_train.toarray()\n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "gb = GaussianNB()\n",
    "gb.fit(X_train_dense, y_train)\n",
    "\n",
    "print(f'Train score: {metrics.accuracy_score(y_train, gb.predict(X_train_dense))}')\n",
    "print(f'Test score: {metrics.accuracy_score(y_test, gb.predict(X_test_dense))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN, Random Forrest, SVM <a id=\"other\"></a>\n",
    "\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lem_stem_df['stem_text']\n",
    "y = lem_stem_df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cvec', CountVectorizer(stop_words='english')),\n",
    "                 ('knn', KNeighborsClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7157190635451505\n",
      "Train score: 0.7385730211817169.\n",
      "Test score: 0.72.\n"
     ]
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [50],\n",
    "    'cvec__min_df': [2],\n",
    "    'cvec__max_df': [.34],\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'knn__n_neighbors': [21],\n",
    "    'knn__metric': ['euclidean']\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_\n",
    "\n",
    "print(f'Train score: {gs.score(X_train, y_train)}.')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds['knn'] = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cvec', CountVectorizer(stop_words='english')),\n",
    "                 ('rf', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8539576365663322\n",
      "Train score: 0.9041248606465998.\n",
      "Test score: 0.845.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.8,\n",
       " 'cvec__max_features': 2000,\n",
       " 'cvec__min_df': 3,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'rf__max_depth': 40,\n",
       " 'rf__min_samples_leaf': 3,\n",
       " 'rf__min_samples_split': 200,\n",
       " 'rf__n_estimators': 700}"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [2000],\n",
    "    'cvec__min_df': [3],\n",
    "    'cvec__max_df': [.8],\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'rf__n_estimators': [700], \n",
    "    'rf__max_depth' : [40],\n",
    "    'rf__min_samples_split': [200], \n",
    "    'rf__min_samples_leaf': [3]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "\n",
    "\n",
    "print(f'Train score: {gs.score(X_train, y_train)}.')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}.')\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds['rf'] = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cvec', CountVectorizer(stop_words='english')),\n",
    "                 ('svc', SVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798216276477146\n",
      "Train score: 0.8857302118171684.\n",
      "Test score: 0.845.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.3,\n",
       " 'cvec__max_features': 300,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'svc__C': 3,\n",
       " 'svc__degree': 0,\n",
       " 'svc__gamma': 0.01,\n",
       " 'svc__kernel': 'rbf'}"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [300],\n",
    "    'cvec__min_df': [2],\n",
    "    'cvec__max_df': [.3],\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'svc__degree' : [0],\n",
    "    'svc__C' : [1, 3 ],\n",
    "    'svc__gamma' : [.001, .01],\n",
    "    'svc__kernel' : ['rbf']\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "\n",
    "\n",
    "print(f'Train score: {gs.score(X_train, y_train)}.')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}.')\n",
    "\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds['svc'] = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('tvec', TfidfVectorizer(stop_words='english')),\n",
    "                 ('knn', KNeighborsClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7363433667781494\n",
      "Train score: 0.7536231884057971.\n",
      "Test score: 0.745.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'knn__leaf_size': 10,\n",
       " 'knn__metric': 'euclidean',\n",
       " 'knn__n_neighbors': 19,\n",
       " 'tvec__max_df': 0.25,\n",
       " 'tvec__max_features': 41,\n",
       " 'tvec__min_df': 2,\n",
       " 'tvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'tvec__max_features': [41],\n",
    "    'tvec__min_df': [2],\n",
    "    'tvec__max_df': [.25],\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    'knn__n_neighbors': [19],\n",
    "    'knn__metric': ['euclidean'],\n",
    "    'knn__leaf_size' : [10]\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "\n",
    "print(f'Train score: {gs.score(X_train, y_train)}.')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}.')\n",
    "\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds['knn2'] = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = preds.drop('class', axis=1).apply(lambda x: np.mean(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds['avg'] = probs>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds['avg'] = preds['avg'].map({False:0, True:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.865"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(preds['class'], preds['avg'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

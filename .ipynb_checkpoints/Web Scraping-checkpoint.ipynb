{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Import Libraries](#import)      \n",
    "- [Scraping Posts from Reddit](#scrape)\n",
    "- [Cleaning Data to Title + Text and Subreddit Only](#clean)\n",
    "- [Saving and Exporting csv for EDA](#save)\n",
    "\n",
    "Keeping scraping in an independent notebook such that when I restart kernels, I do NOT repull data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Importing Libraries <a id=\"import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import requests # Library enables requests from reddit\n",
    "import time # Library enables lag to be added to request loop\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Scraping Posts from Reddit <a id=\"scrape\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up source to scrape from\n",
    "\n",
    "url_base_ww = 'https://www.reddit.com/r/thewestwing.json' # API Endpoint 1\n",
    "url_base_nr = 'https://www.reddit.com/r/Thenewsroom.json' # API Endpoint 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function with a for loop to make pull requests from the sub reddit \n",
    "# until either 1000 posts have been pulled or the entire subreddit has been pulled\n",
    "\n",
    "def pull_posts(url_base):\n",
    "    \n",
    "    # Setting user agent to enable pulling from Reddit\n",
    "    user_agent = {'User-agent': 'mags'}\n",
    "    \n",
    "    # Instantiating 'after' string that can be concatenated to url after the first pull request\n",
    "    after = None\n",
    "    \n",
    "    # Creating empty posts list\n",
    "    posts = []\n",
    "    \n",
    "    for pull_req in range(int(1000/25)):\n",
    "        # Setting url to pull from based on whether it is the first pull (if)\n",
    "        # or a subsequent pull (else)\n",
    "        if after == None:\n",
    "            url = url_base\n",
    "        else:\n",
    "            url = url_base+\"?after=\"+after\n",
    "        \n",
    "        # Making request\n",
    "        res = requests.get(url, headers = user_agent)\n",
    "        \n",
    "        # If statement checks that request worked\n",
    "        # adds posts to posts list.\n",
    "        if res.status_code == 200:\n",
    "            json_data = res.json() \n",
    "            posts.extend(json_data['data']['children'])\n",
    "            # need to change after string such that next pull pulls the next set of posts\n",
    "            after = json_data['data']['after']\n",
    "            \n",
    "        else:\n",
    "            print(f'There was an error : {res.status_code}.')\n",
    "            break\n",
    "        \n",
    "        if after == None:\n",
    "            break\n",
    "        \n",
    "        print(f'Pulled {len(posts)} posts so far...')\n",
    "        time.sleep(5)\n",
    "    \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_ww = pull_posts(url_base_ww)\n",
    "ww_raw_df = pd.DataFrame(posts_ww)\n",
    "ww_raw_df.to_csv('./datasets/raw_ww.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_nr = pull_posts(url_base_nr)\n",
    "nr_raw_df = pd.DataFrame(posts_nr)\n",
    "nr_raw_df.to_csv('./raw_nr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cleaning Data to Title + Text and Subreddit Only <a id=\"clean\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to take the posts and format them such that \n",
    "# the title and text can be used for NLP.\n",
    "\n",
    "# Concatenating title and text because some posts do not have any text,\n",
    "# and some posts have short titles\n",
    "def clean_posts(posts):\n",
    "    # Creating empty list, one for titles and one for text\n",
    "    clean_text = []\n",
    "    \n",
    "    # Looping through each post to create dictionary\n",
    "    for data in posts:\n",
    "        # Creating empty dictionary\n",
    "        text = {}\n",
    "        \n",
    "        text['subreddit'] = data['data']['subreddit']\n",
    "        temp_text =  data['data']['title'] + \" \" + data['data']['selftext']\n",
    "        text = {text['subreddit'] : temp_text}\n",
    "        \n",
    "        clean_text.append(text)\n",
    "    \n",
    "    return pd.DataFrame(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ww = clean_posts(posts_ww)\n",
    "text_ww.to_csv('./titletext_ww.csv')\n",
    "text_ww.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_nr = clean_posts(posts_nr)\n",
    "text_nr.to_csv('./titletext_nr.csv')\n",
    "text_nr.columns = ['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Exporting CSV for EDA  <a id=\"save\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = 1 for West Wing, Y = 0 for Newsroom\n",
    "text_nr['subreddit'] = 0\n",
    "text_ww['subreddit'] = 1\n",
    "\n",
    "# Combining Newsroom and West Wing data frames \n",
    "text_df = text_nr.append(text_ww)\n",
    "text_df['class'] = text_df['subreddit']\n",
    "text_df.drop('subreddit', axis=1, inplace=True)\n",
    "\n",
    "text_df.to_csv('./text_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
